{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from pathlib import Path\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_trump=pd.read_csv('./tweet_data/trump.csv',engine='python')\n",
    "#tweets_biden=pd.read_csv('./tweet_data/biden.csv')\n",
    "#tjb=tweets_biden.dropna(subset=['lat','long','city','state','user_location'],how='all')\n",
    "#tdt=tweets_trump.dropna(subset=['lat','long','city','state','user_location'],how='all')\n",
    "#tjb=tjb.loc[(tjb['country']==\"United States\" ) | (tjb['country']==\"United States of America\")]\n",
    "#tdt=tdt.loc[(tdt['country']==\"United States\" ) | (tdt['country']==\"United States of America\")]\n",
    "#photon_url=\"http://localhost:2322/api?q=\"\n",
    "#photon_reverse=\"http://localhost:2322/reverse?\"\n",
    "\n",
    "#tjb['query_url']=pd.NA\n",
    "#for x in tjb.index:\n",
    "#    if pd.notna(tjb.loc[x,'lat']) and pd.notna(tjb.loc[x,'long']):\n",
    "#        tjb.loc[x,'query_url']=photon_reverse+'lat='+str(tjb.loc[x,'lat'])+'&'+'lon='+str(tjb.loc[x,'long'])\n",
    "#    elif pd.notna(tjb.loc[x,'city']) and pd.notna(tjb.loc[x,'state']):\n",
    "#        tjb.loc[x,'query_url']=photon_url+ tjb.loc[x,'city']+ ' ' + tjb.loc[x,'state']\n",
    "#    else:\n",
    "#        tjb.loc[x,'query_url']=photon_url+str(tjb.loc[x,'user_location'])\n",
    "#tjb['county']=pd.NA\n",
    "#i=0\n",
    "#j=0\n",
    "#for x in tjb.index:\n",
    "#    try:\n",
    "#        photon_response=requests.get(tjb.loc[x,'query_url']).json()['features'][0]['properties']['county']\n",
    "        #j=j+1\n",
    "#    except:\n",
    "#        photon_response=pd.NA\n",
    "#    tjb.loc[x,'county'] = photon_response\n",
    "    #i=i+1\n",
    "    #print(i,j)\n",
    "    #tdt['query_url']=pd.NA\n",
    "#for x in tdt.index:\n",
    "#    if pd.notna(tdt.loc[x,'lat']) and pd.notna(tdt.loc[x,'long']):\n",
    "#        tdt.loc[x,'query_url']=photon_reverse+'lat='+str(tdt.loc[x,'lat'])+'&'+'lon='+str(tdt.loc[x,'long'])\n",
    "#    elif pd.notna(tjb.loc[x,'city']) and pd.notna(tdt.loc[x,'state']):\n",
    "#        tdt.loc[x,'query_url']=photon_url+ tdt.loc[x,'city']+ ' ' + tdt.loc[x,'state']\n",
    "#    else:\n",
    "#        tdt.loc[x,'query_url']=photon_url+str(tdt.loc[x,'user_location'])\n",
    "#tdt['county']=pd.NA\n",
    "#i=0\n",
    "#j=0\n",
    "#for x in tdt.index:\n",
    "#    try:\n",
    "#        photon_response=requests.get(tdt.loc[x,'query_url']).json()['features'][0]['properties']['county']\n",
    "#        j=j+1\n",
    "#    except:\n",
    "#        photon_response=pd.NA\n",
    "#    tdt.loc[x,'county'] = photon_response\n",
    "#    i=i+1\n",
    "#    print(i,j)\n",
    "#filepath=Path('./tweet_data/biden_countied.csv')\n",
    "#tjb.to_csv(filepath)\n",
    "#filepath=Path('./tweet_data/trump_countied.csv')\n",
    "#tdt.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the files\n",
    "#tweets_biden=pd.read_csv('./tweet_data/biden_countied.csv')\n",
    "#tweets_trump=pd.read_csv('./tweet_data/trump_countied.csv')\n",
    "#Only consider those with a valid county and cut down on not needed columns\n",
    "#valid_tweets_b=tweets_biden.dropna(subset=['county']).drop(axis=1,labels=['lat','long','city','country','continent','state','collected_at','user_screen_name','user_name','source','tweet_id','query_url','user_join_date','Unnamed: 0'])\n",
    "#valid_tweets_t=tweets_trump.dropna(subset=['county']).drop(axis=1,labels=['lat','long','city','country','continent','state','collected_at','user_screen_name','user_name','source','tweet_id','query_url','user_join_date','Unnamed: 0'])\n",
    "#valid_tweets=pd.concat([valid_tweets_b,valid_tweets_t], ignore_index=True)\n",
    "#valid_tweets.to_csv('./tweet_data/valid_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is some boilerplate code to cleanup and name the dataset from census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demographics=pd.read_csv('./demographics/demographics.csv')\n",
    "#tmp_in=pd.Series(range(1,39)).apply(str).apply(lambda x:x.zfill(3))\n",
    "#columns_to_drop1=[x+y+z for x in ('S0101_C'+pd.Series(['02','04','05','06'])+'_') for y in tmp_in for z in ['E','M']]\n",
    "#columns_to_drop2=[x+y+z for x in ('S0101_C'+pd.Series(['01','03'])+'_') for y in tmp_in for z in ['M']]\n",
    "#ttmp_in=pd.Series(range(20,39)).apply(str).apply(lambda x:x.zfill(3))\n",
    "#columns_to_drop3=[x+y+z for x in ('S0101_C'+pd.Series(['01','03'])+'_') for y in ttmp_in for z in ['E']]\n",
    "#columns_to_drop=columns_to_drop1+columns_to_drop2+columns_to_drop3+['Unnamed: 458']\n",
    "#().apply(lambda x:x+tmp_in+'E')\n",
    "#len(columns_to_drop)\n",
    "#demographics=demographics.drop(axis=1,labels=columns_to_drop)\n",
    "#def convert_column_names(x):\n",
    "#    x_split = x.split('_')\n",
    "#    if len(x_split)!=3:\n",
    "#        return x\n",
    "#    if x_split[1]=='C01':\n",
    "#        i=int(x_split[2][:-1])\n",
    "#        if i==1:\n",
    "#            return 'total_pop'\n",
    "#        return 'pop_'+str((i-2)*5)+'_'+str((i-1)*5)\n",
    "#    if x_split[1]=='C03':\n",
    "#        i=int(x_split[2][:-1])\n",
    "#        if i==1:\n",
    "#            return 'total_male_pop'\n",
    "#        return 'pop_male_'+str((i-2)*5)+'_'+str((i-1)*5)\n",
    "    \n",
    "#demographics.rename(axis=1,mapper=convert_column_names,inplace=True)\n",
    "#demographics=demographics.loc[1:]\n",
    "#demographics.to_csv('./demographics/demographics_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleanup Poverty Data\n",
    "\n",
    "#poverty=pd.read_csv('./poverty/poverty_rate.csv')\n",
    "#valid_columns=['GEO_ID','NAME','S1701_C01_001E','S1701_C01_006E','S1701_C01_010E','S1701_C01_040E']\n",
    "#poverty_vc=poverty.loc[:,valid_columns]\n",
    "#poverty_vc.rename(axis=1,mapper={'S1701_C01_001E':'tot_poverty','S1701_C01_006E':'pov_18_64','S1701_C01_010E':'pov_65_up','S1701_C01_040E':'pov_150'},inplace=True)\n",
    "#poverty_vc=poverty_vc.loc[1:]\n",
    "#overty_vc.to_csv('./poverty/poverty_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_may_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
